{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "GLOBAL_SEED = 7532\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(GLOBAL_SEED)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(GLOBAL_SEED)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import BatchNormalization, Dense, Dropout, Conv2D, Flatten, MaxPool2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications import VGG16\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('train_set_metadata.csv')\n",
    "valid_set = pd.read_csv('valid_set_metadata.csv')\n",
    "test_set = pd.read_csv('test_set_metadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the main constants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (256, 384, 3)\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "N_EPOCHS = 100\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the f1 metric to be used during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1_Metric(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.f1_scores = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        #score = np.asarray(self.model.predict(self.validation_data[0]))\n",
    "        predict = np.round(np.asarray(self.model.predict(self.validation_data[0])))\n",
    "        targ = self.validation_data[1]\n",
    "\n",
    "        self.f1_scores.append(f1_score(targ, predict, average='weighted'))\n",
    "        \n",
    "        print(f' val_f1: {self.f1_scores[-1]}')\n",
    "        \n",
    "        return\n",
    "    \n",
    "f1_metric = F1_Metric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the class weights to be used during model training in order to fight the class imbalances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.589811320754717,\n",
       " 1: 1.0717714285714286,\n",
       " 2: 1.3650655021834062,\n",
       " 3: 1.5028846153846154,\n",
       " 4: 1.0271631982475355}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_values = train_set['category'].values - 1\n",
    "\n",
    "classes = np.unique(class_values)\n",
    "weights = compute_class_weight('balanced', np.unique(class_values), class_values)\n",
    "\n",
    "class_weights = dict(zip(classes, weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the best model with respect to the f1 score obtained in the first part of the model training. Unfreeze the last 13 layers and compile the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 256, 384, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 256, 384, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 256, 384, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 128, 192, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 128, 192, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 128, 192, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 64, 96, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 64, 96, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 64, 96, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 64, 96, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 32, 48, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 32, 48, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 32, 48, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 32, 48, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 16, 24, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 16, 24, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 16, 24, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 16, 24, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 8, 12, 512)        0         \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 5)                 50596357  \n",
      "=================================================================\n",
      "Total params: 65,311,045\n",
      "Trainable params: 65,050,885\n",
      "Non-trainable params: 260,160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model('model_006_epoch_088_saved.hdf5')\n",
    "\n",
    "for layer in model.layers[-13:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "for layer in model.layers:\n",
    "    print(layer.trainable)\n",
    "\n",
    "# compile the model to reflect the above changes\n",
    "adam = Adam(lr=LEARNING_RATE)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy')    \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model utilizing Adam optimizer, learning rate reduction on plateau, class weights and data augmentation. *Note that the learning was manually stopped after 50 epochs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 212s - loss: 0.8147 - val_loss: 0.5659\n",
      " val_f1: 0.7409163999894015\n",
      "Epoch 2/100\n",
      " - 188s - loss: 0.4328 - val_loss: 0.3066\n",
      " val_f1: 0.8883894859957736\n",
      "Epoch 3/100\n",
      " - 188s - loss: 0.2991 - val_loss: 0.2938\n",
      " val_f1: 0.8882914093962836\n",
      "Epoch 4/100\n",
      " - 188s - loss: 0.3019 - val_loss: 0.3855\n",
      " val_f1: 0.864367028484374\n",
      "Epoch 5/100\n",
      " - 188s - loss: 0.2491 - val_loss: 0.2420\n",
      " val_f1: 0.9131002905905476\n",
      "Epoch 6/100\n",
      " - 188s - loss: 0.2454 - val_loss: 0.4758\n",
      " val_f1: 0.8622775099954675\n",
      "Epoch 7/100\n",
      " - 188s - loss: 0.2507 - val_loss: 0.4666\n",
      " val_f1: 0.8441494555096982\n",
      "Epoch 8/100\n",
      " - 188s - loss: 0.1729 - val_loss: 0.2211\n",
      " val_f1: 0.913030647336355\n",
      "Epoch 9/100\n",
      " - 187s - loss: 0.1430 - val_loss: 0.2642\n",
      " val_f1: 0.9126266592400836\n",
      "Epoch 10/100\n",
      " - 188s - loss: 0.1729 - val_loss: 0.2393\n",
      " val_f1: 0.9175662696639433\n",
      "Epoch 11/100\n",
      " - 187s - loss: 0.1594 - val_loss: 0.2903\n",
      " val_f1: 0.9185588229494767\n",
      "Epoch 12/100\n",
      " - 187s - loss: 0.1215 - val_loss: 0.2257\n",
      " val_f1: 0.9240445515995709\n",
      "Epoch 13/100\n",
      " - 187s - loss: 0.1548 - val_loss: 0.3387\n",
      " val_f1: 0.8568915394091112\n",
      "Epoch 14/100\n",
      " - 187s - loss: 0.0947 - val_loss: 0.1702\n",
      " val_f1: 0.9511603027364591\n",
      "Epoch 15/100\n",
      " - 187s - loss: 0.0543 - val_loss: 0.1819\n",
      " val_f1: 0.9463605634838032\n",
      "Epoch 16/100\n",
      " - 187s - loss: 0.0450 - val_loss: 0.2373\n",
      " val_f1: 0.9410496969291521\n",
      "Epoch 17/100\n",
      " - 187s - loss: 0.0485 - val_loss: 0.1940\n",
      " val_f1: 0.9511159520520449\n",
      "Epoch 18/100\n",
      " - 187s - loss: 0.0336 - val_loss: 0.3831\n",
      " val_f1: 0.9073687265364061\n",
      "Epoch 19/100\n",
      " - 187s - loss: 0.0731 - val_loss: 0.2688\n",
      " val_f1: 0.936397916307147\n",
      "Epoch 20/100\n",
      " - 187s - loss: 0.0307 - val_loss: 0.2132\n",
      " val_f1: 0.9414937861438816\n",
      "Epoch 21/100\n",
      " - 187s - loss: 0.0150 - val_loss: 0.2310\n",
      " val_f1: 0.9503283832235448\n",
      "Epoch 22/100\n",
      " - 187s - loss: 0.0119 - val_loss: 0.2915\n",
      " val_f1: 0.9410087459319073\n",
      "Epoch 23/100\n",
      " - 186s - loss: 0.0141 - val_loss: 0.2187\n",
      " val_f1: 0.9578207954726898\n",
      "Epoch 24/100\n",
      " - 187s - loss: 0.0094 - val_loss: 0.2703\n",
      " val_f1: 0.9464381820982792\n",
      "Epoch 25/100\n",
      " - 187s - loss: 0.0145 - val_loss: 0.2399\n",
      " val_f1: 0.9521040042495598\n",
      "Epoch 26/100\n",
      " - 186s - loss: 0.0059 - val_loss: 0.2572\n",
      " val_f1: 0.9518505208325897\n",
      "Epoch 27/100\n",
      " - 186s - loss: 0.0034 - val_loss: 0.2489\n",
      " val_f1: 0.9507315102895922\n",
      "Epoch 28/100\n",
      " - 186s - loss: 0.0057 - val_loss: 0.2754\n",
      " val_f1: 0.9537990957792054\n",
      "Epoch 29/100\n",
      " - 186s - loss: 0.0048 - val_loss: 0.3110\n",
      " val_f1: 0.9467134561512388\n",
      "Epoch 30/100\n",
      " - 186s - loss: 0.0042 - val_loss: 0.2713\n",
      " val_f1: 0.9506848740641686\n",
      "Epoch 31/100\n",
      " - 186s - loss: 0.0014 - val_loss: 0.2756\n",
      " val_f1: 0.9505313904045044\n",
      "Epoch 32/100\n",
      " - 186s - loss: 0.0022 - val_loss: 0.2878\n",
      " val_f1: 0.9519111649281705\n",
      "Epoch 33/100\n",
      " - 186s - loss: 0.0031 - val_loss: 0.2678\n",
      " val_f1: 0.9524082007511517\n",
      "Epoch 34/100\n",
      " - 186s - loss: 0.0044 - val_loss: 0.2952\n",
      " val_f1: 0.9519140782615095\n",
      "Epoch 35/100\n",
      " - 186s - loss: 0.0019 - val_loss: 0.2745\n",
      " val_f1: 0.9564758510196628\n",
      "Epoch 36/100\n",
      " - 187s - loss: 0.0042 - val_loss: 0.2792\n",
      " val_f1: 0.9540199143864834\n",
      "Epoch 37/100\n",
      " - 186s - loss: 0.0025 - val_loss: 0.2664\n",
      " val_f1: 0.9552884711309712\n",
      "Epoch 38/100\n",
      " - 186s - loss: 0.0027 - val_loss: 0.2675\n",
      " val_f1: 0.9568760024506676\n",
      "Epoch 39/100\n",
      " - 186s - loss: 0.0025 - val_loss: 0.2631\n",
      " val_f1: 0.9548014441880882\n",
      "Epoch 40/100\n",
      " - 186s - loss: 0.0020 - val_loss: 0.2685\n",
      " val_f1: 0.9561028679497198\n",
      "Epoch 41/100\n",
      " - 186s - loss: 6.4418e-04 - val_loss: 0.2722\n",
      " val_f1: 0.9570982819976854\n",
      "Epoch 42/100\n",
      " - 186s - loss: 0.0021 - val_loss: 0.2628\n",
      " val_f1: 0.9552419197751719\n",
      "Epoch 43/100\n",
      " - 186s - loss: 0.0014 - val_loss: 0.2693\n",
      " val_f1: 0.955846300796228\n",
      "Epoch 44/100\n",
      " - 186s - loss: 0.0014 - val_loss: 0.2722\n",
      " val_f1: 0.9568426754249426\n",
      "Epoch 45/100\n",
      " - 186s - loss: 8.0085e-04 - val_loss: 0.2749\n",
      " val_f1: 0.9571626713836296\n",
      "Epoch 46/100\n",
      " - 186s - loss: 5.1758e-04 - val_loss: 0.2756\n",
      " val_f1: 0.9577930087678578\n",
      "Epoch 47/100\n",
      " - 186s - loss: 0.0012 - val_loss: 0.2858\n",
      " val_f1: 0.9552858063229221\n",
      "Epoch 48/100\n",
      " - 186s - loss: 0.0015 - val_loss: 0.2769\n",
      " val_f1: 0.9571595741777414\n",
      "Epoch 49/100\n",
      " - 186s - loss: 0.0012 - val_loss: 0.2816\n",
      " val_f1: 0.9572031210562156\n",
      "Epoch 50/100\n",
      " - 187s - loss: 0.0012 - val_loss: 0.2774\n",
      " val_f1: 0.9568725945882591\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7209e7f5f072>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     33\u001b[0m                     \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weights\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m                     \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m                     verbose=2)\n\u001b[0m",
      "\u001b[1;32mc:\\users\\rafal_pikula_01\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rafal_pikula_01\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1418\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rafal_pikula_01\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rafal_pikula_01\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rafal_pikula_01\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rafal_pikula_01\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\rafal_pikula_01\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare the training and validation data\n",
    "X_train = np.load('train_set_hmgd_arr_256_384_VGG16.npy')\n",
    "y_train = to_categorical(train_set['category'].values - 1)\n",
    "\n",
    "X_valid = np.load('valid_set_hmgd_arr_256_384_VGG16.npy')\n",
    "y_valid = to_categorical(valid_set['category'].values - 1)\n",
    "\n",
    "\n",
    "# Initialize callbacks\n",
    "checkpoint = ModelCheckpoint('model_010_FT001_epoch_{epoch:03d}.hdf5', monitor='val_loss', \n",
    "                             save_best_only=False, save_weights_only=False)\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=12)    \n",
    "callback_list = [f1_metric, checkpoint, lr_reduction]#, early_stopping]\n",
    "\n",
    "\n",
    "# Fit the model\n",
    "#model.fit(X_train, y_train, epochs=N_EPOCHS, batch_size=BATCH_SIZE, class_weight=class_weights, \n",
    "#          callbacks=callback_list, validation_data=(X_valid, y_valid))\n",
    "\n",
    "data_gen = ImageDataGenerator(rotation_range=20, \n",
    "                              width_shift_range=0.2, \n",
    "                              height_shift_range=0.2, \n",
    "                              horizontal_flip=True)\n",
    "\n",
    "steps_per_epoch = int(len(X_train)) / BATCH_SIZE\n",
    "\n",
    "model.fit_generator(data_gen.flow(X_train, y_train, batch_size=BATCH_SIZE), \n",
    "                    steps_per_epoch=steps_per_epoch, \n",
    "                    epochs=N_EPOCHS,\n",
    "                    callbacks=callback_list,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    class_weight=class_weights,\n",
    "                    workers=4, \n",
    "                    verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Free up memory and read in the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_valid\n",
    "\n",
    "X_test = np.load('test_set_hmgd_arr_256_384_VGG16.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the best 2 models with respect to the f1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_01 = load_model('model_010_FT001_epoch_023.hdf5')\n",
    "model_02 = load_model('model_010_FT001_epoch_046.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the predictions and prepare the submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission = sample_submission.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob_01 = model_01.predict(X_test)\n",
    "pred_prob_02 = model_02.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_classes_01 = pred_prob_01.argmax(axis=-1) + 1\n",
    "pred_classes_02 = pred_prob_02.argmax(axis=-1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    913\n",
       "2    516\n",
       "5    512\n",
       "3    389\n",
       "4    350\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['category'] = pred_classes_01\n",
    "submission['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('model_010_submission_01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    926\n",
       "5    515\n",
       "2    503\n",
       "3    391\n",
       "4    345\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission['category'] = pred_classes_02\n",
    "submission['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('model_010_submission_02.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                      Epoch 23      Epoch 46\n",
    "# Validation score:  0.9578207955  0.9577930088\n",
    "# Leaderboard score: 0.9617234462  0.9609120265"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
